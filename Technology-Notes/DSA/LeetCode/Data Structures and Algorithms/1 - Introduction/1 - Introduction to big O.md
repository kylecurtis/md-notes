
Before we talk about big O, it's important that we first understand what exactly an "algorithm" is, especially in the context of LeetCode.

An algorithm can be seen as a recipe for a computer to follow. It's a set of instructions that a computer will follow step-by-step to solve a problem.

Algorithms take an input and produce an output. The output will be the answer to a question regarding the input. For example, let's say you had a non-empty array of positive integers calledÂ `nums`, and you wanted to answer the question: "What is the largest number inÂ `nums`?".

To answer this question, you would write an algorithm that takes an array calledÂ `nums`Â asÂ **input**, andÂ **outputs**Â the largest number inÂ `nums`. Here is an example of such an algorithm:

1. Create a variableÂ `maxNum`Â and initialize it toÂ `0`.
2. Iterate over each elementÂ `num`Â inÂ `nums`.
3. IfÂ `num`Â is greater thanÂ `maxNum`, updateÂ `maxNum = num`.
4. OutputÂ `maxNum`.

Here, we have written down a set of instructions that when followed, will solve the problem. We can now implement these instructions in code so that a computer can quickly solve the problem. There are some important requirements for algorithms in the context of LeetCode:

- Algorithms should beÂ **deterministic**. Given the sameÂ **input**, the algorithm shouldÂ **always**Â produce the sameÂ **output**. Basically, there shouldn't be any randomness.
- The algorithm should be correct for any arbitrary validÂ **input**. In our example, we said thatÂ `nums`Â is a non-empty array of positive integers. There are infinitely many of such arrays, and our algorithm works forÂ **all**Â of them. Note that ifÂ `nums`Â had negative numbers, the input would be invalid since we stated the integers are positive. In fact, our algorithm would actually break because we initializedÂ `maxNum`Â toÂ `0`, so if all ofÂ `nums`Â was negative, we would incorrectly outputÂ `0`.

<br>

---

<br>

## Big O

Big O is a notation used to describe the computational complexity of an algorithm. The computational complexity of an algorithm is split into two parts: time complexity and space complexity. The time complexity of an algorithm is the amount of time the algorithm needs to run relative to the input size. The space complexity of an algorithm is the amount of memory allocated by the algorithm when run relative to the input size.

> Typically, people care about the time complexity more than the space complexity, but both are important to know.

Time complexity: as the input size grows, how much longer does the algorithm take to complete?

Space complexity: as the input size grows, how much more memory does the algorithm use?

<br>

---

<br>

## How complexity works

Complexity is described by a function (math formula). What should the arguments to this function be?

The arguments are variables defined by the programmer, but they should represent values that change between different inputs, and these values should affect the algorithm. For example, the most common variable you'll see isÂ $n$, which usually denotes the length of an input array or string. In the example above, we could say thatÂ $n$Â is equal to the length ofÂ `nums`.

Here, "the length ofÂ `nums`" is a value that changes between inputs, and it directly affects the algorithm. The longerÂ `nums`Â is, the more elements we need to iterate through, and thus the longer our algorithm will take to complete.

> Note that choosing the letterÂ $n$Â to denote this value is arbitrary. There is no requirement that we useÂ $n$, it's just thatÂ $n$Â is the commonly accepted standard that everyone uses. If you wanted, you could use a banana (ðŸŒ) to represent the length ofÂ `nums`. The programmer is the one who decides which variables represent what values.

In the context of LeetCode, there are some common assumptions that we make. When dealing with integers, the larger the integer, the more time operations like addition, multiplication, or printing will take. While thisÂ **is**Â relevant in theory, we typically ignore this fact because the difference is practically very small, and treat all integers the same. If you are given an array of integers as an input, the only variable you would use isÂ $n$Â to denote the length of the array. Technically, youÂ _could_Â introduce another variable, let's sayÂ $k$Â which denotes the average value of the integers in the array. However, nobody does this.

When written, the function is wrapped by a capital O. Here are some example complexities:

- $O(n)$
- $O(n^2)$
- $O(2^n)$
- $O(log n)$
- $O(n * m)$

> You might be thinking, what isÂ ï¿½m? Remember: we define the variables. As these are simply examples with no associated problem,Â $m$Â could denote any arbitrary variable. For example, we could have a problem where the input is two arrays.Â $n$Â could denote the length of one whileÂ $m$Â denotes the length of the other.

These functions represent the complexity. For example, you would say "The time complexity of my algorithm is $O(n)$" or "The space complexity of my algorithm is $O(n^2)$".

<br>

---

<br>

## Calculating complexity

Roughly, your function calculates the number of operations or amount of memory (depending on if you're analyzing time or space complexity, respectively) your algorithm consumes relative to the input size. Using the example from above (find the largest number inÂ `nums`), we have a time complexity ofÂ $O(n)$. The algorithm involves iterating over each element inÂ `nums`, so if we defineÂ ï¿½nÂ as the length ofÂ `nums`, our algorithm uses approximatelyÂ $n$Â steps. If we pass an array with a length ofÂ `10`, it will perform approximatelyÂ `10`Â steps. If we pass an array with a length ofÂ `10,000,000,000`, it will perform approximatelyÂ `10,000,000,000`Â steps.

> Time complexity is not meant to be anÂ **exact**Â representation of the number of operations. For example, we needed to initializeÂ `maxNum = 0`Â and we also needed to outputÂ `maxNum`Â at the end. Thus, you could argue that for an array of lengthÂ `10`, we needÂ `12`Â operations. ThisÂ **is not the point**Â of time complexity. The point of time complexity is to describe how the number of operations changes as the input changes. The number of iterations we do depends onÂ `nums`, but initializingÂ `maxNum = 0`Â doesn't.

Being able to analyze an algorithm and calculate its time and space complexity is a crucial skill. Interviewers willÂ **almost always**Â ask you for your algorithm's complexity to check that you actually understand your algorithm and didn't just memorize/copy the code. Being able to analyze an algorithm also enables you to determine what parts of it can be improved.

**Rules**

There are a few rules when it comes to calculating complexity. First,Â **we ignore constants**. That meansÂ $O(9999999n)=O(8n)=O(n)=O(\frac{n}{500})$. Why do we do this? Imagine you had two algorithms. Algorithm A uses approximatelyÂ $n$Â operations and algorithm B uses approximatelyÂ $5n$Â operations.

WhenÂ $n=100$, algorithm A usesÂ $100$Â operations and algorithm B usesÂ $500$Â operations. What happens if we doubleÂ $n$? Then algorithmÂ $A$Â usesÂ $200$Â operations and algorithm $B$ usesÂ $1000$Â operations. As you can see, when we double the value ofÂ $n$, both algorithms require double the amount of operations. If we were to 10x the value ofÂ $n$, then both algorithms would require 10x more operations.

Remember: the point of complexity is to analyze the algorithmÂ **as the input changes**. We don't care that algorithm B is 5x slower than algorithm A. For both algorithms, as the input size increases, the number of operations required increasesÂ **linearly**. That's what we care about. Thus, both algorithms areÂ $O(n)$.

The second rule is that we consider the complexity as the variablesÂ **tend to infinity**. When we have addition/subtraction between terms of theÂ **same**Â variable, we ignore all terms except the most powerful one. For example,Â $O(2^n+n^2-500n)=O(2^2)$. Why? Because asÂ $n$Â tends to infinity,Â $2n$Â becomes so large that the other two terms are effectively zero in comparison.

Let's say that we had an algorithm that requiredÂ $n+500$Â operations. It has a time complexity ofÂ $O(n)$. WhenÂ $n$Â is small, let's sayÂ $n=5$, theÂ $+500$Â term is very significant - but we don't care about that. We need to perform the analysis as ifÂ $n$Â is tending toward infinity, and in that scenario, theÂ $500$Â is nothing.

> The best complexity possible isÂ $O(1)$, called "constant time" or "constant space". It means that the algorithm ALWAYS uses the same amount of resources, regardless of the input.
> 
> Note that a constant time complexity doesn't necessarily mean that an algorithm is fast ($O(5000000)=O(1)$), it just means that its runtime is independent of the input size.

When talking about complexity, there are normally three cases:

- Best case scenario
- Average case
- Worst case scenario

In most algorithms, all three of these will be equal, but some algorithms will have them differ. If you have to choose only one to represent the algorithm's time or space complexity, never choose the best case scenario. It is most correct to use the worst case scenario, but you should be able to talk about the difference between the cases.

<br>

---

<br>

## Analyzing time complexity

Let's look at some example algorithms in pseudo-code and talk about their time complexities.

```cpp
// Given an integer array "arr" with length n,
for (int num: arr) {
    print(num)
}
```


This algorithm has a time complexity ofÂ $O(n)$. In each for loop iteration, we are performing a print, which costsÂ $O(1)$. The for loop iteratesÂ $n$Â times, which gives a time complexity ofÂ $O(1 \cdot n)=O(n)$.

<br>

---

<br>

```cpp
// Given an integer array "arr" with length n,
for (int num: arr) {
    for (int i = 0; i < 500,000; i++) {
        print(num)
    }
}
```

This algorithm has a time complexity ofÂ $O(n)$. In each inner for loop iteration, we are performing a print, which costsÂ $O(1)$. This for loop iterates 500,000 times, which means each outer for loop iteration costsÂ $O(500000)=O(1)$. The outer for loop iteratesÂ $n$Â times, which gives a time complexity ofÂ $O(n)$.

Even though the first two algorithmsÂ _technically_Â have the same time complexity, in reality the second algorithm isÂ **much**Â slower than the first one. It's correct to say that the time complexity isÂ $O(n)$, but it's important to be able to discuss the differences between practicality and theory.

<br>

---

<br>

```cpp
// Given an integer array "arr" with length n,
for (int num: arr) {
    for (int num2: arr) {
        print(num * num2)
    }
}
```

This algorithm has a time complexity ofÂ $O(n^2)$. In each inner for loop iteration, we are performing a multiplication and print, which both costÂ $O(1)$. The inner for loop runsÂ $n$Â times, which means each outer for loop iteration costsÂ $O(n)$. The outer for loop runsÂ $O(n)$Â times, which gives a time complexity ofÂ $O(n \cdot n)=O(n^2)$.

<br>

---

<br>

```cpp
// Given integer arrays "arr" with length n and "arr2" with length m,
for (int num: arr) {
    print(num)
}

for (int num: arr) {
    print(num)
}

for (int num: arr2) {
    print(num)
}
```

This algorithm has a time complexity ofÂ $O(n+m)$. The first two for loops both costÂ $O(n)$, whereas the final for loop costsÂ $O(m)$. This gives a time complexity ofÂ $O(2n+m)$Â =Â $O(n+m)$.

<br>

---

<br>

```cpp
// Given an integer array "arr" with length n,
for (int i = 0; i < arr.length; i++) {
    for (int j = i; j < arr.length; j++) {
        print(arr[i] + arr[j])
    }
}
```

This algorithm has a time complexity ofÂ $O(n2)$. The inner for loop is dependent on what iteration the outer for loop is currently on. The first time the inner for loop is run, it runsÂ $n$Â times. The second time, it runsÂ $nâˆ’1$Â times, thenÂ $nâˆ’2$,Â $nâˆ’3$, and so on.

That means the total iterations is 1 + 2 + 3 + 4 + ... + n, which is the partial sum ofÂ [this series](https://en.wikipedia.org/wiki/1_%2B_2_%2B_3_%2B_4_%2B_%E2%8B%AF#Partial_sums), which is equal toÂ $\frac{n \cdot (n+1)}{2} = \frac{n^2+n}{2}$â€‹. In big O, this isÂ $O(n^2)$Â because the addition term in the numerator and the constant term in the denominator are both ignored.

<br>

---

<br>

**Logarithmic time**

A logarithm is the inverse operation to exponents. The time complexityÂ $O(log n)$Â is called logarithmic time and isÂ **extremely**Â fast. A common time complexity isÂ $O(n \cdot log n$), which is reasonably fast for most problems and also the time complexity of efficient sorting algorithms.

Typically, the base of the logarithm will beÂ `2`. This means that if your input is sizeÂ `n`, then the algorithm will performÂ `x`Â operations, whereÂ 2ï¿½=ï¿½2x=n. However, the base of the logarithmÂ [doesn't actually matter](https://stackoverflow.com/questions/1569702/is-big-ologn-log-base-e/1569710#1569710)Â for big O, since all logarithms are related by a constant factor.

$O(log n)$Â means that somewhere in your algorithm, the input is being reduced by a percentage at every step. A good example of this is binary search, which is a searching algorithm that runs inÂ $O(log n)$Â time (there is a chapter dedicated to binary search later on). With binary search, we initially consider the entire input (`n`Â elements). After the first step, we only considerÂ `n / 2`Â elements. After the second step, we only considerÂ `n / 4`Â elements, and so on. At each step, we are reducing our search space by 50%, which gives us a logarithmic time complexity.

<br>

---

<br>

## Analyzing space complexity

When you initialize variables like arrays or strings, your algorithm is allocating memory. We never count the space used by the input (it is bad practice to modify the input), and usually don't count the space used by the output (the answer) unless an interviewer asks us to.

> In the below examples, the code is only allocating memory so that we can analyze the space complexity, so we will consider everything we allocate as part of the space complexity (there is no "answer").

```cpp
// Given an integer array "arr" with length n

for (int num: arr) {
    print(num)
}
```

This algorithm has a space complexity ofÂ $O(1)$. The only space allocated is an integer variableÂ `num`, which is constant relative toÂ $n$.

<br>

---

<br>

```cpp
// Given an integer array "arr" with length n
Array doubledNums = int[]

for (int num: arr) {
    doubledNums.add(num * 2)
}
```

This algorithm has a space complexity ofÂ $O(n)$. The arrayÂ `doubledNums`Â storesÂ $n$Â integers at the end of the algorithm.

<br>

---

<br>

```cpp
// Given an integer array "arr" with length n

Array nums = int[]
int oneHundredth = n / 100

for (int i = 0; i < oneHundredth; i++) {
    nums.add(arr[i])
}
```

This algorithm has a space complexity ofÂ $O(n)$. The arrayÂ `nums`Â stores the first 1% of numbers inÂ `arr`. This gives a space complexity ofÂ $O(\frac{n}{100})=O(n)$.

<br>

---

<br>

```cpp
// Given integer arrays "arr" with length n and "arr2" with length m,

Array grid = int[n][m]

for (int i = 0; i < arr.length; i++) {
    for (int j = 0; j < arr2.length; j++) {
        grid[i][j] = arr[i] * arr2[j]
    }
}
```

This algorithm has a space complexity ofÂ $O(n \cdot m)$. We are creating aÂ `grid`Â that has dimensionsÂ $n \cdot m$.

> In this course, we will talk extensively about time and space complexity. If it's a new concept to you, don't worry - with practice, you will become more and more comfortable with analyzing algorithms on your own.

